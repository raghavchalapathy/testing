\addvspace {10\p@ }
\contentsline {table}{\numberline {1.1}{\ignorespaces Comparison of our Survey to Other Related Survey Articles. 1 \textemdash Our Survey, 2 \textemdash Kwon and Donghwoon \nobreakspace {}\cite {kwon2017survey}, 5 \textemdash John and Derek \nobreakspace {}\cite {ball2017comprehensive} 3 \textemdash Kiran and Thomas \nobreakspace {}\cite {kiran2018overview}, 6 \textemdash Mohammadi and Al-Fuqaha \nobreakspace {}\cite {mohammadi2017deep} 4 \textemdash Adewumi and Andronicus \nobreakspace {}\cite {adewumi2017survey} 7 \textemdash Geert and Kooi et.al \nobreakspace {}\cite {litjens2017survey}. \relax }}{5}{table.caption.15}
\contentsline {table}{\numberline {1.2}{\ignorespaces Table illustrating nature of input data and corresponding deep anomaly detection model architectures proposed in literature. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks AE: Autoencoders. \relax }}{8}{table.caption.17}
\contentsline {table}{\numberline {1.3}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in HIDS CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks GRU: Gated Recurrent Unit, DNN : Deep Neural Networks SPN: Sum Product Networks\relax }}{17}{table.caption.24}
\contentsline {table}{\numberline {1.4}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in NIDS. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks RNN: Recurrent Neural Networks, RBM : Restricted Boltzmann Machines DCA: Dilated Convolution Autoencoders, CVAE : Convolutional Variational Autoencoder AE: Autoencoders, SAE: Stacked Autoencoders , DBN : Deep Belief Network GAN: Generative Adversarial Networks. \relax }}{17}{table.caption.25}
\contentsline {table}{\numberline {1.5}{\ignorespaces Datasets Used in Intrusion Detection \relax }}{18}{table.caption.26}
\contentsline {table}{\numberline {1.6}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used for credit card fraud detection. AE: Autoencoders, LSTM : Long Short Term Memory Networks RBM: Restricted Botlzmann Machines, DNN : Deep Neural Networks GRU: Gated Recurrent Unit, RNN: Recurrent Neural Networks CNN: Convolutional Neural Networks,VAE: Variational Autoencoders GAN: Generative Adversarial Networks\relax }}{20}{table.caption.28}
\contentsline {table}{\numberline {1.7}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used for mobile cellular network fraud detection. CNN: convolution neural networks,DBN: Deep Belief Networks SAE: Stacked Autoencoders, DNN : Deep neural networks GAN: Generative Adversarial Networks \relax }}{21}{table.caption.29}
\contentsline {table}{\numberline {1.8}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used for insurance fraud detection. DBN: Deep Belief Networks, DNN : Deep Neural Networks CNN: Convolutional Neural Networks,VAE: Variational Autoencoders GAN: Generative Adversarial Networks\relax }}{22}{table.caption.30}
\contentsline {table}{\numberline {1.9}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used for health care fraud detection. RBM: Restricted Botlzmann Machines, GAN: Generative Adversarial Networks\relax }}{22}{table.caption.31}
\contentsline {table}{\numberline {1.10}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used for malware detection. AE: Autoencoders, LSTM : Long Short Term Memory Networks RBM: Restricted Botlzmann Machines, DNN : Deep Neural Networks GRU: Gated Recurrent Unit, RNN: Recurrent Neural Networks CNN: Convolutional Neural Networks,VAE: Variational Autoencoders GAN: Generative Adversarial Networks,CNN-BiLSTM: CNN- Bidirectional LSTM\relax }}{23}{table.caption.32}
\contentsline {table}{\numberline {1.11}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used for medical anomaly detection. AE: Autoencoders, LSTM : Long Short Term Memory Networks GRU: Gated Recurrent Unit, RNN: Recurrent Neural Networks CNN: Convolutional Neural Networks,VAE: Variational Autoencoders GAN: Generative Adversarial Networks, KNN: K-nearest neighbours RBM: Restricted Boltzmann Machines.\relax }}{24}{table.caption.33}
\contentsline {table}{\numberline {1.12}{\ignorespaces Examples of Deep learning anomaly detection techniques used to detect anomalies in social network. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks AE: Autoencoders, DAE: Denoising Autoencoders SVM : Support Vector Machines., DNN : Deep Neural Network \relax }}{25}{table.caption.34}
\contentsline {table}{\numberline {1.13}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in system logs. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks GRU: Gated Recurrent Unit, DNN : Deep Neural Networks AE: Autoencoders, DAE: Denoising Autoencoders\relax }}{26}{table.caption.35}
\contentsline {table}{\numberline {1.14}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in Internet of things (IoT) Big Data Anomaly Detection. AE: Autoencoders, LSTM : Long Short Term Memory Networks DBN : Deep Belief Networks.\relax }}{27}{table.caption.36}
\contentsline {table}{\numberline {1.15}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in industrial operations. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks GRU: Gated Recurrent Unit, DNN : Deep Neural Networks AE: Autoencoders, DAE: Denoising Autoencoders, SVM: Support Vector Machines SDAE: Stacked Denoising Autoencoders, RNN : Recurrent Neural Networks.\relax }}{28}{table.caption.37}
\contentsline {table}{\numberline {1.16}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in time series data. CNN: Convolution Neural Networks, GAN: Generative Adversarial networks,LSTM : Long Short Term Memory Networks GRU: Gated Recurrent Unit, DNN : Deep Neural Networks, AE: Autoencoders, DAE: Denoising Autoencoders, VAE: Variational Autoencoder SDAE: Stacked Denoising Autoencoders\relax }}{29}{table.caption.38}
\contentsline {table}{\numberline {1.17}{\ignorespaces Examples of Deep learning anomaly detection Techniques Used in video surveillance. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks RBM: Restricted Boltzmann Machine, DNN : Deep Neural Networks, CAE: Convolutional Autoencoders AE: Autoencoders, DAE: Denoising Autoencoders, OCSVM: One class Support vector machines SDAE: Stacked Denoising Autoencoders, STN : Spatial Transformer Networks \relax }}{31}{table.caption.39}
\contentsline {table}{\numberline {1.18}{\ignorespaces Semi-supervised deep anomaly detection models overview AE: Autoencoders, DAE: Denoising Autoencoders, KNN : K- Nearest Neighbours CorGAN: Corrupted Generative Adversarial Networks, DBN: Deep Belief Networks AAE: Adversarial Autoencoders, CNN: Convolution neural networks SVM: Support vector machines.\relax }}{34}{table.caption.40}
\contentsline {table}{\numberline {1.19}{\ignorespaces Examples of Hybrid Deep learning anomaly detection Techniques. CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks DBN: Deep Belief Networks, DNN : Deep Neural Networks. AE: Autoencoders, DAE: Denoising Autoencoders, SVM: Support Vector Machines SVDD: Support Vector Data Description, RNN : Recurrent Neural Networks Relief: Feature selection Algorithm, KNN: K- Nearest Neighbours CSI: Capture, Score, and Integrate. \relax }}{36}{table.caption.41}
\contentsline {table}{\numberline {1.20}{\ignorespaces Examples of Un-supervised Deep Anomaly Detection (UDAD). CNN: Convolution Neural Networks, LSTM : Long Short Term Memory Networks DNN : Deep Neural Networks., GAN: Generative Adversarial Network AE: Autoencoders, DAE: Denoising Autoencoders, SVM: Support Vector Machines STN: Spatial Transformer Networks, RNN : Recurrent Neural Networks AAE: Adversarial Autoencoders, VAE : Variational Autoencoders.\relax }}{40}{table.caption.42}
\addvspace {10\p@ }
\contentsline {table}{\numberline {2.1}{\ignorespaces Example sentence in a DNR task with entity classes represented in IOB format.\relax }}{53}{table.caption.44}
\contentsline {table}{\numberline {2.2}{\ignorespaces Statistics of training and test datasets used for SemEval-2013 Task 9.1.\relax }}{53}{table.caption.45}
\contentsline {table}{\numberline {2.3}{\ignorespaces Example sentence in a CE task with concept classes represented in IOB format.\relax }}{61}{table.caption.46}
\contentsline {table}{\numberline {2.4}{\ignorespaces Statistics of training and test datasets used for 2010-i2b2 concept extraction.\relax }}{62}{table.caption.47}
\contentsline {table}{\numberline {2.5}{\ignorespaces Performance comparison between the bidirectional LSTM CRF (bottom three lines) and state-of-the-art systems (top five lines) over the 2010 i2b2/VA concept extraction task.\relax }}{64}{table.caption.48}
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of datasets used in experiments.\relax }}{74}{table.caption.49}
\contentsline {table}{\numberline {3.2}{\ignorespaces Summary of best performing feed-forward network architecture's used in OC-NN model for experiments.\relax }}{76}{table.caption.51}
\contentsline {table}{\numberline {3.3}{\ignorespaces Average AUCs in \% with StdDevs (over 10 seeds) per method on MNIST and CIFAR-10 dataset.\relax }}{80}{table.caption.58}
\contentsline {table}{\numberline {3.4}{\ignorespaces Average AUCs in \% with StdDevs (over 10 seeds) per method on GTSRB stop signs with adversarial attacks.\relax }}{81}{table.caption.59}
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Summary of datasets used in experiments.\relax }}{95}{table.caption.62}
