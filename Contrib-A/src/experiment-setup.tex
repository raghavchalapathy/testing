\section{Experimental Setup}  \label{sec:experiment-setup}
% !TEX root=../main.tex
\subsection{Datasets}
The DDIExtraction 2013 shared task challenge from SemEval-2013 Task 9.1~\cite{segura2013semeval} has provided a benchmark corpus for DNR and DDI extraction. The corpus contains manually-annotated pharmacological substances and drug-drug interactions (DDIs) for a total of $18,502$ pharmacological substances and $5,028$ DDIs. It collates two distinct datasets: DDI-DrugBank and DDI-MedLine~\cite{herrero2013ddi}. Table~\ref{table2} summarizes the basic statistics of the training and test datasets used in our experiments. For proper comparison, we follow the same settings as \cite{segura2015exploring}, using the training data of the DNR task along with the test data for the DDI task for training and validation of DNR. We split this joint dataset into a training and validation sets with approximately $70\%$ of sentences for training and the remaining for validation.

\subsection{Evaluation Methodology}
Our models have been blindly evaluated on unseen DNR test data using the \textit{strict} evaluation metrics. With this evaluation, the predicted entities have to match the ground-truth entities exactly, both in boundary and class. To facilitate the replication of our experimental results, we have used a publicly-available library for the implementation\footnote{\tt https://github.com/raghavchalapathy/dnr} (i.e., the Theano neural network toolkit \cite{bergstra2010theano}). The experiments have been run over a range of values for the hyper-parameters, using the validation set for selection~\cite{bergstra2012random}. The hyper-parameters include the number of hidden-layer nodes, $H \in \{25, 50, 100\}$, the context window size, $s \in \{1, 3, 5\}$, and the embedding dimension, $d \in \{50, 100, 300, 500, 1000\}$. Two additional parameters, the learning and drop-out rates, were sampled from a uniform distribution in the range $[0.05, 0.1]$. The embedding and initial weight matrices were all sampled from the uniform distribution within range $[-1, 1]$. Early training stopping was set to $100$ epochs to mollify over-fitting, and the model that gave the best performance on the validation set was retained. The accuracy is reported in terms of micro-average F$_1$ score computed using the CoNLL score function~\cite{Nadeau:07}.

