% !TEX root=../main.tex
\section{Related Work}
\label{unsupervisedDAD:RelatedWork}

Consider a feature matrix $\X \in \Real^{N \times D}$,
where $N$ denotes the number of data points and $D$ the number of features for each point.
For example, $N$ could be the number of images in some photo collection, and $D$ the number of pixels used to represent each image.
The goal of anomaly detection is to determine which rows of $\X$ are anomalous, in the sense of being dissimilar to all other rows.
We will use $\X_{i :}$ to denote the $i$th row of $\X$.

%
\subsection{A tour of anomaly detection methods}

Anomaly detection is a widely researched topic in the data mining and machine learning community~\cite{chandola2007outlier,charubook}.
The two primary strands of research have been the design of novel algorithms to detect anomalies,
and the design \emph{efficient} means of discovering all anomalies in a large dataset.
In the latter strand, starting from the work of Bay and Schwabacher~\cite{bay03}, several optimisations have been proposed to discover anomalies in near linear time~\cite{Ghoting:2008}.

In the former strand, which is our primary focus, most emphasis has been on non-parametric methods like distance and density based outliers~\cite{knorr1997unified,breunig2000lof}.
For example, distance-based methods define a domain-dependent dissimilarity metric, and deem a point to be anomalous if it is relatively far away from its neighbours~\cite{Zhao:2009}.
Another popular approach is the one-class SVM, which learns a smooth boundary that captures the majority of probability mass of the data~\cite{Scholkopf:2001}.


In recent years,
matrix factorization methods for anomaly detection have become popular.
These methods provide a \emph{reconstruction matrix} $\hat{\X} \in \Real^{N \times D}$ of the input $\X$, and use the norm $\| \X_{i :} - \hat{\X}_{i :} \|_2^2$ as a measure of how anomalous a particular point $\X_{i :}$ is;
if the reconstruction is close to the input, then it is deemed normal;
else, anomalous.
We describe several popular examples of this approach, beginning with principal component analysis (PCA).



%
\subsection{PCA for anomaly detection}

PCA finds the directions of maximal variance of the data.
Supposing without loss of generality that the data matrix $\X$ has zero mean,
this may be understood as the result of a matrix factorisation \cite{Bishop:2006}:
\begin{equation}
	\label{eqn:pca}
	\min_{\W^T \W = \I, \Z} \| \X - \W \Z \|_F^2 = \min_{\U} \| \X - \X \U \U^T \|_F^2.
\end{equation}
Here,
the reconstruction matrix is $\hat{\X} = \X \U \U^T$,
where
$\U \in \Real^{D \times K}$ for some number of \emph{latent dimensions} $K \ll D$.
We can interpret $\X \U$ as a projection (or encoding) of $\X$ into a $K$-dimensional subspace,
with the application of $\U^T$ as an inverse projection (or decoding) back into the original $D$ dimensional space.


%
\subsection{Autoencoders for anomaly detection}

PCA assumes a linear subspace explains the data.
To relax this assumption, consider instead
\begin{equation}
	\label{eqn:autoencoder}
	\min_{\U, \V} \| \X - f( \X \U ) \V \|_F^2
\end{equation}
for some non-decreasing \emph{activation function} $f \colon \Real \to \Real$,
and $\U \in \Real^{D \times K}, \V \in \Real^{K \times D}$.
For the purposes of anomaly detection, one can define the reconstruction matrix as $\hat{\X} = f( \X \U ) \V$.

Equation \ref{eqn:autoencoder} corresponds to an autoencoder with a single hidden layer \cite{Goodfellow-et-al-2016}.
Popular choices of $f( \cdot )$ include the sigmoid $f( a ) = (1 + \exp(-a))^{-1}$ and the rectified linear unit or ReLU $f( x ) = \max(0, a)$.
As before, we can interpret $\X \U$ as an encoding of $\X$ into a $K$-dimensional subspace; however,
by applying a nonlinear $f( \cdot )$,
the projection is implicitly onto a nonlinear manifold.


%
\subsection{Robust PCA}

Another way to generalise PCA is to solve, for a tuning parameter $\lambda > 0$,
\begin{equation}
	\label{eqn:robust-pca}
	\min_{\S, \N} \| \S \|_* + \lambda \cdot \| \N \|_1 : \X = \S + \N,
\end{equation}
where $\| \cdot \|_*$ denotes the trace or nuclear norm $\| \X \|_* = \mathrm{tr}( (\X^T \X)^{1/2} )$,
and $\| \cdot \|_1$ the elementwise $\ell_1$ norm.
For the purposes of anomaly detection, one can define the reconstruction matrix $\hat{\X} = \X - \N = \S$.

Intuitively, Equation \ref{eqn:robust-pca} separates $\X$ into a signal matrix $\S$ and a noise matrix $\N$,
where the signal matrix has low-rank structure, and the noise is assumed to not overwhelm the signal for most of the matrix entries.
The trace norm may be seen as a convex relaxation of the rank function;
thus, this objective can be understood as a relaxed version of PCA.

Equation \ref{eqn:robust-pca} corresponds to robust PCA (RPCA)~\cite{candes2010robust}.
Unlike standard PCA, this objective can effortlessly deal with a single entry perturbed arbitrarily.
When $\lambda \to +\infty$, we will end up with $\N = \mathbf{0}, \S = \X$,
i.e.\, we will claim that there is no noise in the data, and so all points are deemed normal.
On the other hand, when $\lambda \to 0$, we will end up with $\N = \X, \S = \mathbf{0}$,
i.e.\, we will claim that there is no signal in the data, and so points with high norm are deemed anomalous.


%
\subsection{Direct robust matrix factorization}

Building upon RPCA,
Xiong et. al.~\cite{xiong2011direct} introduced the direct robust matrix factorization method (DRMF),
where for tuning parameters $K, e$ one solves:
\begin{equation}
	\label{eqn:drmf}
	\begin{aligned}
	\min_{\S, \N} \hspace{0.5cm} & \| \X - (\N + \S) \|_{F}^2 \colon \mbox{rank}(\S) \leq K, \|\N\|_{0} \leq e.
	\end{aligned}
\end{equation}
As before, the matrix $\N$ captures the anomalies and $\S$ captures the signal.
Unlike RPCA, one explicitly constraints $\S$ to be low-rank, rather than merely having low trace norm;
and one explicitly constraints $\N$ to have a maximal number of nonzeros, rather than merely having bounded $\ell_1$ norm.
The lack of convexity of the objective requires a bespoke algorithm for the optimisation.


%
\subsection{Robust kernel PCA}

Another way to overcome the linear assumption of PCA is the robust kernel PCA (RKPCA) approach of~\cite{Nguyen:2009}.
For a feature mapping $\Upphi$ into a reproducing kernel Hilbert space, and projection operator $\mathbf{P}$ of a point into the KPCA subspace, it is proposed to reconstruct an input $\mathbf{x} \in \Real^D$ by solving the pre-image problem
\begin{equation}
	\label{eqn:rkpca}
	\hat{\mathbf{x}} = \underset{\mathbf{z} \in \Real^D}{\mathrm{argmin}} \, E_0( \mathbf{x}, \mathbf{z} ) + C \cdot \| \Upphi( \mathbf{z} ) - \mathbf{P} \Upphi( \mathbf{z} ) \|^2,
\end{equation}
where $E_0$ is a robust measure of reconstruction error (i.e.\ not merely the Euclidean norm),
and $C > 0$ is a tuning parameter.
RKPCA does not explicitly handle gross outliers, unlike RPCA;
however, by choosing a rich feature mapping $\Upphi$, %(e.g.\ corresponding to a Gaussian kernel),
one can capture nonlinear anomalies.
This choice of feature mapping must be pre-specified, whereas autoencoder methods implicitly \emph{learn} a good mapping.
