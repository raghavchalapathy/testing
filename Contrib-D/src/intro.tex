% !TEX root=../main.tex
\chapter{Unsupervised Deep Anomaly Detection Methods}
\label{chpt:unsupervisedDAD}
\section{Introduction}

A common need when analysing real-world datasets is determining which instances stand out as being dramatically dissimilar to all others.
Such instances are known as \emph{anomalies}, and the goal of \emph{anomaly detection} (also known as \emph{outlier detection}) is to determine all such instances in a data-driven fashion~\cite{chandola2007outlier}.
Anomalies can be caused by errors in the data but sometimes are indicative of a new, previously unknown, underlying process;
in fact Hawkins~\cite{hawkins} defines an outlier as an observation that {\it deviates so significantly from other observations as to arouse suspicion that it was generated by a different mechanism.}

Principal Component Analysis (PCA) \cite{Hotelling:1933} is a core method for a range of statistical inference tasks, including anomaly detection.
The basic idea of PCA is that while many data sets are high-dimensional, they tend to inhabit a {low-dimensional manifold}.
PCA thus operates by (linearly) projecting data into a lower-dimensional space, so as to separate the {\em signal} from the {\em noise};
a data point which is far away from its projection is deemed as anomalous.

While intuitive and popular, PCA has limitations as an anomaly detection method.
Notably, it is highly sensitive to data perturbation: one extreme data point can completely change the orientation of the projection, often leading to the masking of anomalies.
A variant of PCA, known as a \emph{robust} PCA (RPCA) limits the impact of anomalies by using a clever decomposition of the data matrix~\cite{candes2010robust}.
We will discuss RPCA in detail in Section~\ref{sec:background},
but note here that it still carries out a linear projection,
and further cannot be used to make predictions on test instances;
that is, we cannot perform \emph{inductive} anomaly detection.

In this paper, we will relax the linear projection limitation of RPCA by using a deep and robust autoencoder~\cite{vincent2010stacked,Goodfellow-et-al-2016}.
The difference between RPCA and a deep autoencoder will be the use of a nonlinear activation function and the potential use of several hidden layers in the autoencoder.
While this modification is conceptually simple, we show it yields noticeable improvements in anomaly detection performance on complex real-world image data, where a linear projection cannot capture sufficient structure in the data.
Further, the robust autoencoder is capable of performing inductive anomaly detection, unlike RPCA.

\begin{comment}
Figure~\ref{mse} shows the output of using a robust and deep autencoder
to recover images. The data set consists of images of dogs where each
image has been corrupted with a small amount of noise. The proposed
autoencoder is able to reconstruct the dog images but fails to
properly reconstruct an image which has a dog and a boy. In fact,
the image of the dog with the boy was discovered as part of the
anomaly detection process using autoencoders.
\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.5]{images/otherClass}
	\caption{Illustration of the  anomaly detection capability of deep inductive convolutional autoencoders.
		The data set consists of ``dog'' images (first column).
		Our proposed robust autoencoder decomposes an image $\X = \hat{\X} + \N$.
		The $\hat{\X}$ (second column) shows the reconstructed image and $\N$ (third column) shows the difference between the original and the reconstructed image.
		In the first row, a dog image is normal, while in the second row, an image of a flight  (an anomaly) is  reconstructed with high mean square error.}
	\label{mse}
	\vspace{-\baselineskip}
\end{figure}
\end{comment}

In the sequel,
we provide an overview of anomaly detection methods (Section~\ref{sec:related}), with a specific emphasis on matrix decomposition techniques
such as PCA and its robust extensions.
We then proceed to describe our proposed model based on autoencoders (Section~\ref{sec:method}),
and present our experiment setup and results (Section~\ref{sec:experiment-setup}, \ref{sec:experiment-results}).
Finally, we describe directions for future work (Section~\ref{sec:conclusion}).
