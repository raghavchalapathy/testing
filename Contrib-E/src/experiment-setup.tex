\section{Experimental Setup}  \label{sec:experiment-setup} 
% !TEX root=../main.tex
In this section, we show the empirical effectiveness of deep generative models (DGMs) over the state-of-the-art methods on real-world data. Our primary focus is on non-trivial image datasets, although our method is applicable in any context where autoencoders are useful e.g. speech, text.

\subsection{Methods compared}
 We evaluate our proposed  technique using DGMs with  state-of-the art GAD methods. Our comparison study  for detecting group anomalies involves:
\let\labelitemi\labelitemii
\begin{itemize}

	\item \textbf{Mixture of Gaussian Mixture (MGM)  Model}, as per~\cite{MGM}.
	\item \textbf{One-Class Support Measure Machines (OCSMM)}, as per~\cite{OCSMM}.
	\item \textbf{One-Class Support Vector Machines (OCSVM)}, as per~\cite{OCSVM}.
 	\item \textbf{Variational Autoencoder (VAE)}~\cite{doersch2016tutorial}, as per Equation (\ref{eqn:vaeloss}).
	\item \textbf{Adversarial Autoencoder (AAE)}~\cite{makhzani2015adversarial}, as per Equation (\ref{eqn:aaeloss}).
\end{itemize}

We used Keras~\cite{chollet2015keras}, TensorFlow \cite{abadi2016tensorflow} for the implementation of AAE and  VAE~\footnote{\url{https://github.com/raghavchalapathy/gad}}. MGM \footnote{\url{https://www.cs.cmu.edu/~lxiong/gad/gad.html}}, OCSMM
\footnote{\url{https://github.com/jorjasso/SMDD-group-anomaly-detection}}
and OCSVM 
\footnote{\url{
https://github.com/cjlin1/libsvm}}
are applied using publicly available code. %\footnote{\url{http://perception.csl.illinois.edu/matrix-rank/sample_code.html}}\footnote{\url{http://www3.cs.stonybrook.edu/~minhhoai/downloads.html}}.

%\vspace{-0.3 cm}
\subsection{Datasets}
%%\vspace{-0.2 cm}
Our data experiments are conducted on the following datasets: % (as illustrated in Figure \ref{fig:GAD}):
\begin{itemize}
{  \item {\tt synthetic} data follows Muandet et al. \cite{OCSMM} where regular groups are generated by bivariate Gaussian distributions while anomalous groups have  rotated covariance matrices. }
	\item {\tt cifar-10}~\cite{krizhevsky2009learning} consists of $32\times32$ color images over 10 classes with 6000 images per class.
    \item {\tt scene} image data following Xiong et al. ~\cite{FGM} where anomalous images are stitched from different scene categories.
 {  \item {\tt Pixabay}~\cite{pixabayImages} is used to obtain tiger images as well as images of cats and dogs together. These images are  rescaled to match dimensions of cat images in {\tt cifar-10} dataset. 
 }
 \end{itemize} 
The real-world data experiments %using {\tt cifar-10}  and {\tt scene}  data 
are previously illustrated in Figure~\ref{fig:GAD}. 
 
 

\subsection{Parameter Selection} 
We now briefly discuss the model and parameter selection for applying techniques in GAD applications.  A pre-processing stage is required for state-of-the-art GAD methods when dealing with images %. Firstly images are converted to grayscale and 
where feature extraction methods such as SIFT \cite{sift} or HOG \cite{hog}  represent images as a collection of visual features. %  After the feature extraction stage,  principal component analysis reduces visual features for each image to 2 dimensions.
In MGM, the number of regular group behaviours $T$ and number of Gaussian mixtures $L$ are selected using information criteria.  The kernel bandwidth smoothing parameter  in OCSMM \cite{OCSMM} is chosen as $  \mbox{median}\big\{ || {\bf G}_{m,i} -{\bf G}_{l,j} ||^2 \big\} $ for all $i,j \in \{1,2,\dots,N_m \}$ and $m,l \in {1,2,\dots,M}$ where $ {\bf G}_{m,i} $ represents the $i$th observation in the $m$th group.  In addition, the parameter for expected  proportions of anomalies in  OCSMM and OCSVM is set to the true value in the respective datasets. 
%Although we have observed that deeper adversarial, variational  autoencoder networks tend to achieve better group anomaly detection performance, there exist four fold options related to network parameters to be chosen:

When applying VAE and AAE, there are four existing  network parameters that require careful selection;
(a) number of convolutional filters, (b) filter size, (c) strides of convolution operation and (d) activation function. We tuned via grid search of additional hyper-parameters  including the number of hidden-layer nodes $H \in \{3, 64, 128\}$ and regularisation  $\lambda$ within range ${[0, 100]}$. The learning drop-out rates and regularisation parameter $\mu$ were sampled from a uniform distribution in the range $[0.05, 0.1]$. The embedding and initial weight matrices are all sampled from uniform distribution within range $[-1, 1]$.
