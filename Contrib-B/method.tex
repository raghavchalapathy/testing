\section{Method}
\label{sec:method}
CE can be formulated as a joint segmentation and classification task over a predefined set of classes. As an example, consider the input sentence provided in Table~\ref{table1}. The notation follows the widely adopted in/out/begin (IOB) entity representation with, in this instance, \textit{HCT} as the test, \textit{2U PRBC} as the treatment. In this paper, we approach the CE task by bidirectional LSTM CRF and we therefore provide a brief description hereafter. In a bidirectional LSTM CRF, each word in the input sentence is first mapped to a random real-valued  vector of arbitrary dimension, $d$. Then, a measurement for the word, noted as $x(t)$, is formed by concatenating the word's own vector with a window of preceding and following vectors (the ``context''). An example of input vector with a context window of size $s = 3$ is:
\begin{equation}
  \begin{split}
    w_{3}(t) = [His, \textbf{HCT}, dropped], \\
    `His' \rightarrow x_{HCT} \in \mathbb{R}^{d}, \\
    `HCT' \rightarrow x_{His} \in \mathbb{R}^{d}, \\
    `dropped' \rightarrow x_{dropped} \in \mathbb{R}^{d}, \\
    x(t) = [x_{His}, x_{\textbf{HCT}}, x_{dropped}] \in \mathbb{R}^{3d}
  \end{split}
\end{equation}

\noindent where $w_{3}(t)$ is the context window centered around the $t$-th word, $'HCT'$, and $x_{word}$ represents the numerical vector for $word$.


\subsection{Word Embeddings}
Word embeddings are  dense vector  representations of natural language words that preserves the semantic and syntactic similarities between them. The vector representations could be generated by either count based such as Hellinger-PCA ~\cite{lebret2013word}, direct prediction models such as Word2Vec comprising of Skip-gram or Common Bag of Words (CBOW) or Glove word embeddings. Glove vector representations captures complex patterns beyond word similarity through by combining  efficient use of word co-occurance statistics and generate a global vector representation for any given word.

%This is achieved  through representing words as high-dimensional vectors: the spatial relationship between these vector representations subsequently  capture  the semantic relationships among words.



\subsection{Bidirectional LSTM-CRF Networks}

The LSTM was designed to overcome this limitation by incorporating a gated memory-cell to capture long-range dependencies within the data~\cite{hochreiter1997long}. In the bidirectional LSTM, for any given sentence, the network computes both a left, $\overrightarrow{h}(t)$, and a right, $\overleftarrow{ h}(t)$, representations of the sentence context at every input, $x(t)$. The final representation is created by concatenating them as $h(t) = [\overrightarrow{h}(t)$;$\overleftarrow{ h}(t)]$. All these networks utilize the $h(t)$ layer as an implicit feature for entity class prediction: although this model has proved effective in many cases, it is not able to provide joint decoding of the outputs in a Viterbi-style manner (e.g., an I-group cannot follow a B-brand; etc). Thus, another modification to the bidirectional LSTM is the addition of a conditional random field (CRF)~\cite{lafferty2001conditional} as the output layer to provide optimal sequential decoding. The resulting network is commonly referred to as the bidirectional LSTM-CRF \cite{lample2016neural}.



\begin{table*}[ht]
  \centering

    \begin{tabular}{|c|c|c|c|}
      \hline
      \multirow{3}{*}{Methods} &
      \multicolumn{3}{c|}{\bf {\small 2010 i2b2/VA}} \\
      \cline{2-4}
      & Precision & Recall & F$_1$ Score \\
      \hline
      semi-supervised Markov HMM \cite{de2011machine} &$86.88$ &$83.64$ & $85.23$ \\
      % \hline
      distributonal semantics-CRF \cite{jonnalagadda2012enhancing} &$85.60$&$82.00$ &$83.70$  \\
      binarized neural embedding CRF\cite{wu2015study}&$85.10$&$80.60$ & $82.80$ \\
      CliNER \cite{boagcliner}&$79.50$&$81.20$ & $80.00$ \\
      truecasing CRFSuite \cite{fu2014improving}&$80.83$&$7 1.47$ & $75.86$ \\
      \hline
      \bf(Our Approach) & & &  \\
      random-bidirectional LSTM-CRF &$00.00$ &$00.00$ & $78.13$ \\
      Word2Vec-bidirectional LSTM-CRF &$00.00$ &$00.00$ & $81.30$ \\
      Glove-bidirectional LSTM-CRF &$00.00$ &$00.00$ & $83.81$ \\
      \hline
    \end{tabular}
    \caption{Performance comparison between the bidirectional LSTM CRF (bottom three lines) and state-of-the-art systems (top five lines) over the 2010 i2b2/VA concept extraction task.}
    \label{table3}
  \end{table*}


