\section{ Experiment Setup}
\label{Sec:experimentsetup}
\subsection{Datasets}
\label{sec:length}
The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks, one among them is  concept extraction task focused on the extraction of medical concepts from patient  reports. A total of 394 training reports, 477 test reports, and 877 unannotated reports were de-identified and released to challenge participants with data use agreements~\cite{uzuner20112010}. However part of that data set is no longer being distributed due to Institutional Review Board (IRB) restrictions. Table~\ref{table2} summarizes the basic statistics of the training and test datasets used in our experiments. We split training dataset into a training and validation sets with approximately $70\%$ of sentences for training and the remaining for validation.

\subsection{Evaluation Methodology}
\label{sec:length}
Our models have been blindly evaluated on unseen 2010 i2b2/VA  CE test data using the \textit{strict} evaluation metrics. With this evaluation, the predicted entities have to match the ground-truth entities exactly, both in boundary and class. To facilitate the replication of our experimental results, we have used a publicly-available library for the implementation (i.e., the Theano neural network toolkit \cite{bergstra2010theano}) and we publicly release our code\footnote{\hspace{-0.65cm}https://github.com/raghavchalapathy/Bidirectional-LSTM-CRF-for-Clinical-Concept-Extraction}. The experiments have been run over a range of values for the hyper-parameters, using the validation set for selection~\cite{bergstra2012random}. The hyper-parameters include the number of hidden-layer nodes, $H \in \{25, 50, 100\}$, the context window size, $s \in \{1, 3, 5\}$, and the embedding dimension, $d \in \{50, 100, 300, 500, 1000\}$. Two additional parameters, the learning and drop-out rates, were sampled from a uniform distribution in the range $[0.05, 0.1]$. To begin with, the embedding and initial weight matrices were all randomly initialized from the uniform distribution within range $[-1, 1]$ subsequently word embeddings with $d= 300$ derived from Word2Vec and Glove was utilized in the experiments. Early training stopping was set to $100$ epochs to mollify over-fitting, and the model that gave the best performance on the validation set was retained. The accuracy is reported in terms of micro-average F$_1$ score computed using the CoNLL score function~\cite{Nadeau:07}.
