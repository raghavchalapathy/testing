\section{Problem Introduction}
\label{sec:conceptExtraction}
Patient clinical records contain  longitudinal record of patient health, disease, test's conducted  and response to treatment, often useful for epidemiologic and clinical research. Thus extracting these information  has been of immense value for both clinical practise and to improve  quality of patient care provided  while reducing healthcare costs. Concept extraction (CE) aims to identify medical concept mentions such as problems, test, treatments in  clinical  records (Eg: discharge summaries, progress reports) and classify them into pre-defined categories. The concepts in  clinical records are often expressed with unstructured free text, rendering their extraction a daunting task for  clinical Natural Language Processing (NLP) systems. The CE  problem is analogous to well-studied Named Entity Recognition (NER) task in general NLP domain. Traditional approaches to extracting concepts relied on  rule based systems or dictionaries (lexicon's) using string comparision to recognise concepts of interest. The concepts  represent drug names, anatomical nomenclature, other specialised names and phrases which are not part of mundane English vocabulary. For instance "resp status" should be interpreted as "response status". Furthermore the use of abbreviated phrases are very common among medical fraternity and many of these abbreviations have alternative meanings in other genres of English. Intrinsically, rule based systems are hard to scale, and ineffective in the presence of informal sentences and abbreviated phrases~\cite{liu2015drug}. Dictionary based systems perform a  fast look-up  from medical ontologies such as Unified Medical Language System (UMLS)  to extract concepts~\cite{kipper2008system}. Although these systems achieve high precison but suffer from low recall ( i,e they may not identify significant number of concepts) due to missplelled words or medical jargons not present in dictionaries. To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants  have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines~(SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup~\cite{lafferty2001conditional,berger1996maximum,joachims1998text}. However these state-of-the-art ML approaches follow two step process of domain specific feature engineering and classification, which are highly dedicated hand-crafted systems and require labour intensive expert knowledge. For this reason, this paper employs bidirectional LSTM-CRF intialized with  general purpose off-the-shelf neural word embeddings derived from Glove~\cite{Pennington:14} and Word2Vec~\cite{Mikolov:13}  for automatic feature learning thus avoiding time-consuming feature engineering,  which deliver system performance comparable to the best submissions from the 2010 i2b2/VA challenge.
